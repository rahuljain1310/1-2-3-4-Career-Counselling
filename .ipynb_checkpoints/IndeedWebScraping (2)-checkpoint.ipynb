{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Import the required libraries\n",
    "import bs4\n",
    "import urllib3\n",
    "import csv\n",
    "import numpy as np\n",
    "urllib3.disable_warnings()\n",
    "http = urllib3.PoolManager()\n",
    "\n",
    "# ntlk\n",
    "from nltk.tokenize import sent_tokenize, word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from string import punctuation\n",
    "from nltk.probability import FreqDist"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Sanitizing and Sorting Output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def sanitize_input(data):\n",
    "    replace = {\n",
    "        ord('\\f') : ' ', \n",
    "        ord('\\t') : ' ',\n",
    "        ord('\\n') : ' ',\n",
    "        ord('\\r') : None\n",
    "    }\n",
    "    return data.translate(replace)\n",
    "\n",
    "def remove_html_tags(text):\n",
    "    \"\"\"Remove html tags from a string\"\"\"\n",
    "    import re\n",
    "    clean = re.compile('<.*?>')\n",
    "    return re.sub(clean, '', text)\n",
    "\n",
    "def tokenize_content(content):\n",
    "    stop_words = set(stopwords.words('english') + list(punctuation))\n",
    "    words = word_tokenize(content.lower())\n",
    "    return [word for word in words if word not in stop_words]\n",
    "\n",
    "def word_freq(content):\n",
    "    filtered_words = tokenize_content(content)\n",
    "    word_freq = FreqDist(filtered_words)\n",
    "    return word_freq\n",
    "\n",
    "def filterWords(allWords):\n",
    "    removewords = ['p','div','while','total','been','e','our','like','new','which','help','s','all','some','if','what','about','only','on','then','will','no','at','a','for','us','not','etc','we','that','it','the','of','as','an','may','have','has','this','other','from','with','its','be','in','is','am','now','you','some','was','can','are','but','they','he','she','where','when','and','or','them','how','by','to']\n",
    "    selWords = [word for word in allWords if word not in removewords]\n",
    "    selWords = [word for word in selWords if not str.isdigit(word)]\n",
    "    return selWords\n",
    "def filtercharacter(string):\n",
    "    removechar = ['\"',\"'\",'?',',','‘','’','-','(',')',':','—','/','<','>']\n",
    "    string = [c if c not in removechar else ' ' for c in string ]\n",
    "    return ''.join(string)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Link of Pages with list of JDs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def url_query(job):\n",
    "    Url_List = []\n",
    "    job = job.strip().replace(\" \",\"+\")\n",
    "    base_url = 'https://www.indeed.co.in/jobs?l=India&q='\n",
    "    job_url_default = base_url+job\n",
    "    Url_List.append(job_url_default)\n",
    "    job_url_page = job_url_default+'&start='\n",
    "    for i in range(1,2):             \n",
    "        Url_List.append(job_url_page+str(10*i))\n",
    "    return Url_List"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Extract JD from given List"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def JDfrompage (page_url):\n",
    "    r = http.request('GET', page_url)\n",
    "    source = r.data\n",
    "    soup = bs4.BeautifulSoup(source, \"lxml\")\n",
    "    all_div = [div for div in soup.findAll('div',{\"class\": \"jobsearch-SerpJobCard\"}) ]\n",
    "    all_jk = ['https://www.indeed.co.in/viewjob?jk='+div.get('data-jk') for div in all_div]\n",
    "    return all_jk"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### All JD of given job"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def get_JDs(JD_URL_List):    \n",
    "    res = []\n",
    "    for url in JD_URL_List:\n",
    "        print(url)\n",
    "        r = http.request('GET',url)\n",
    "        source = r.data\n",
    "        if(r.status==200):\n",
    "            soup = bs4.BeautifulSoup(source, \"lxml\")\n",
    "            JD = soup.find('div',{\"class\": \"jobsearch-JobComponent-description\"})\n",
    "            JD_content = JD.contents[0].encode('utf-8').decode()\n",
    "            JD_content = sanitize_input(JD_content)\n",
    "            JD_content = remove_html_tags(JD_content)\n",
    "            res.append(JD_content)\n",
    "    return res"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Creating list of all jobs and Skills"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "Jobtype = [\"Android Developer\"\n",
    "           ,\"Web Developer\"\n",
    "           ,\"Consultant\"\n",
    "           ,\"Finance\"\n",
    "           ,\"Data Scientist\"\n",
    "           ,\"Business Analyst\"\n",
    "           ,\"Designer\"\n",
    "           ,\"Mechanical Engineer\"\n",
    "           ,\"Electrical Engineer\"\n",
    "           ,\"Civil Engineer\"\n",
    "           ,\"Chemical Engineer\"\n",
    "           ,\"Software Developer\"\n",
    "           ,\"Game Developer\"]\n",
    "Skills = ['Mumbai'\n",
    "         ,'Delhi'\n",
    "         ,'Bangalore'\n",
    "         ,'Kochi'\n",
    "         ,'Noida']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "JobAndJDs = {}\n",
    "for job in Jobtype:\n",
    "    JD_URL_List = []\n",
    "    for url in url_query(job):\n",
    "        x = JDfrompage(url)   # list of all jobs from that url\n",
    "        JD_URL_List = JD_URL_List+x\n",
    "    all_JD = get_JDs(JD_URL_List)\n",
    "    JobAndJDs[job] = all_JD\n",
    "    print(job+ \":- Total JDs = \"+str(len(all_JD)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def skillinJd(jd,skills):\n",
    "    skillMatrix = []\n",
    "    for c in skills:\n",
    "        if(jd.count(c)>0):\n",
    "            skillMatrix.append(1.0)\n",
    "        else:\n",
    "            skillMatrix.append(0.0)\n",
    "#     print(np.array(skillMatrix))\n",
    "    return np.array(skillMatrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "CareerSkillMatrix = {}\n",
    "for career in JobAndJDs.keys():\n",
    "    SkillMatrix = np.array([0]*len(Skills))\n",
    "    for jd in JobAndJDs[career]:\n",
    "        SkillMatrix = np.add(SkillMatrix,skillinJd(jd,Skills))\n",
    "    SkillMatrix = np.divide(SkillMatrix,len(JobAndJDs[c]))\n",
    "    CareerSkillMatrix[career] = SkillMatrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# All_Resume = {}\n",
    "# x = (\"Machine learning is an application of artificial intelligence (AI) that provides systems the ability to automatically learn and improve from experience without being explicitly programmed. Machine learning focuses on the development of computer programs that can access data and use it learn for themselves. The process of learning begins with observations or data, such as examples, direct experience, or instruction,\"\n",
    "#      \"in order to look for patterns in data and make better decisions in the future based on the examples that we provide. The primary aim is to allow the computers learn automatically without human intervention or assistance and adjust actions accordingly. Some machine learning methods Machine learning algorithms are often categorized as supervised or unsupervised. Supervised machine learning algorithms can apply what ha\"\n",
    "#      \"s been learned in the past to new data using labeled examples to predict future events. Starting from the analysis of a known training dataset, the learning algorithm produces an inferred function to make predictions about the output values. The system is able to provide targets for any new input after sufficient training. The learning algorithm can also compare its output with the correct, intended output and find \"\n",
    "#      \"errors in order to modify the model accordingly. In contrast, unsupervised machine learning algorithms are used when the information used to train is neither classified nor labeled. Unsupervised learning studies how systems can infer a function to describe a hidden structure from unlabeled data. The system doesn’t figure out the right output, but it explores the data and can draw inferences from datasets to describe\"\n",
    "#      \"hidden structures from unlabeled data. Semi-supervised machine learning algorithms fall somewhere in between supervised and unsupervised learning, since they use both labeled and unlabeled data for training – typically a small amount of labeled data and a large amount of unlabeled data. The systems that use this method are able to considerably improve learning accuracy. Usually, semi-supervised learning is chosen when\"\n",
    "#      \" the acquired labeled data requires skilled and relevant resources in order to train it / learn from it. Otherwise, acquiringunlabeled data generally doesn’t require additional resources. Reinforcement machine learning algorithms is a learning method that interacts with its environment by producing actions and discovers errors or rewards. Trial and error search and delayed reward are the most relevant characteristics of\"\n",
    "#      \"reinforcement learning. This method allows machines and software agents to automatically determine the ideal behavior within a specific context in order to maximize its performance. Simple reward feedback is required for the agent to learn which action is best; this is known as the reinforcement signal.\"\n",
    "#      \"Machine learning enables analysis of massive quantities of data. While it generally delivers faster, more accurate results in order to identify profitable opportunities or dangerous risks, it may also require additional time and resources to train it properly. Combining machine learning with AI and cognitive technologies can make it even more effective in processing large volumes of information.\")\n",
    "# y = (\"Machine learning Vaniyambadi, Tamil Nadu Secure a responsible career opportunity to fully utilize my training and skills, while making a significant contribution to the success of the company\"\n",
    "#      \"Work Experience Machine learning traing Present first position Education BCA in art Concordia Higher Secondary School Vaniyambadi, Tamil Nadu 2016 to 2019 S.S.L.C in State Board Concordia Higher Secondary School Vaniyambadi, Tamil Nadu\"\n",
    "#      \"2014 Skills C++ (Less than 1 year), CSS (Less than 1 year), HTML (Less than 1 year), JAVA (Less than 1 year), JAVASCRIPT (Less than 1 year) Additional Information Technical Skill • Hardware And Networking • Typing Lower • C, C++, JAVA, VISUAL PROGRAMMING, JAVASCRIPT, HTML, CSS\"\n",
    "#      \"• OS-LINUX BASIC OPERTORS\")\n",
    "# All_Resume['Machine Learning'] = [x,y]\n",
    "# All_Resume['Deep Learning'] = [x,y]\n",
    "# Total_Resume = 4\n",
    "All_Resume = {}\n",
    "Total_Resume = 0"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
