{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Import the required libraries\n",
    "import bs4\n",
    "import urllib3\n",
    "import csv\n",
    "import numpy as np\n",
    "urllib3.disable_warnings()\n",
    "http = urllib3.PoolManager()\n",
    "\n",
    "# ntlk\n",
    "from nltk.tokenize import sent_tokenize, word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from string import punctuation\n",
    "from nltk.probability import FreqDist"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Sanitizing and Sorting Output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def sanitize_input(data):\n",
    "    replace = {\n",
    "        ord('\\f') : ' ', \n",
    "        ord('\\t') : ' ',\n",
    "        ord('\\n') : ' ',\n",
    "        ord('\\r') : None\n",
    "    }\n",
    "    return data.translate(replace)\n",
    "\n",
    "def remove_html_tags(text):\n",
    "    \"\"\"Remove html tags from a string\"\"\"\n",
    "    import re\n",
    "    clean = re.compile('<.*?>')\n",
    "    return re.sub(clean, '', text)\n",
    "\n",
    "def tokenize_content(content):\n",
    "    stop_words = set(stopwords.words('english') + list(punctuation))\n",
    "    words = word_tokenize(content.lower())\n",
    "    return [word for word in words if word not in stop_words]\n",
    "\n",
    "def word_freq(content):\n",
    "    filtered_words = tokenize_content(content)\n",
    "    word_freq = FreqDist(filtered_words)\n",
    "    return word_freq\n",
    "\n",
    "def filterWords(allWords):\n",
    "    removewords = ['p','div','while','total','been','e','our','like','new','which','help','s','all','some','if','what','about','only','on','then','will','no','at','a','for','us','not','etc','we','that','it','the','of','as','an','may','have','has','this','other','from','with','its','be','in','is','am','now','you','some','was','can','are','but','they','he','she','where','when','and','or','them','how','by','to']\n",
    "    selWords = [word for word in allWords if word not in removewords]\n",
    "    selWords = [word for word in selWords if not str.isdigit(word)]\n",
    "    return selWords\n",
    "def filtercharacter(string):\n",
    "    removechar = ['\"',\"'\",'?',',','‘','’','-','(',')',':','—','/','<','>']\n",
    "    string = [c if c not in removechar else ' ' for c in string ]\n",
    "    return ''.join(string)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Link of Pages with list of JDs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def url_query(job):\n",
    "    Url_List = []\n",
    "    job = job.strip().replace(\" \",\"+\")\n",
    "    base_url = 'https://www.indeed.co.in/jobs?l=India&q='\n",
    "    job_url_default = base_url+job\n",
    "    Url_List.append(job_url_default)\n",
    "    job_url_page = job_url_default+'&start='\n",
    "    for i in range(1,2):             \n",
    "        Url_List.append(job_url_page+str(10*i))\n",
    "    return Url_List"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Extract JD from given List"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def JDfrompage (page_url):\n",
    "    r = http.request('GET', page_url)\n",
    "    source = r.data\n",
    "    soup = bs4.BeautifulSoup(source, \"lxml\")\n",
    "    all_div = [div for div in soup.findAll('div',{\"class\": \"jobsearch-SerpJobCard\"}) ]\n",
    "    all_jk = ['https://www.indeed.co.in/viewjob?jk='+div.get('data-jk') for div in all_div]\n",
    "    return all_jk"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### All JD of given job"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def get_JDs(JD_URL_List):    \n",
    "    res = []\n",
    "    for url in JD_URL_List:\n",
    "        print(url)\n",
    "        r = http.request('GET',url)\n",
    "        source = r.data\n",
    "        if(r.status==200):\n",
    "            soup = bs4.BeautifulSoup(source, \"lxml\")\n",
    "            JD = soup.find('div',{\"class\": \"jobsearch-JobComponent-description\"})\n",
    "            JD_content = JD.contents[0].encode('utf-8').decode()\n",
    "            JD_content = sanitize_input(JD_content)\n",
    "            JD_content = remove_html_tags(JD_content)\n",
    "            res.append(JD_content)\n",
    "    return res"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Creating list of all jobs and Skills"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "Jobtype = [\"Android Developer\"\n",
    "           ,\"Web Developer\"\n",
    "           ,\"Consultant\"\n",
    "           ,\"Finance\"\n",
    "           ,\"Data Scientist\"\n",
    "           ,\"Business Analyst\"\n",
    "           ,\"Designer\"\n",
    "           ,\"Mechanical Engineer\"\n",
    "           ,\"Electrical Engineer\"\n",
    "           ,\"Civil Engineer\"\n",
    "           ,\"Chemical Engineer\"\n",
    "           ,\"Software Developer\"\n",
    "           ,\"Game Developer\"]\n",
    "Skills = ['Mumbai'\n",
    "         ,'Delhi'\n",
    "         ,'Bangalore'\n",
    "         ,'Kochi'\n",
    "         ,'Noida']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "JobAndJDs = {}\n",
    "for job in Jobtype:\n",
    "    JD_URL_List = []\n",
    "    for url in url_query(job):\n",
    "        x = JDfrompage(url)   # list of all jobs from that url\n",
    "        JD_URL_List = JD_URL_List+x\n",
    "    all_JD = get_JDs(JD_URL_List)\n",
    "    JobAndJDs[job] = all_JD\n",
    "    print(job+ \":- Total JDs = \"+str(len(all_JD)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def skillinJd(jd,skills):\n",
    "    skillMatrix = []\n",
    "    for c in skills:\n",
    "        if(jd.count(c)>0):\n",
    "            skillMatrix.append(1.0)\n",
    "        else:\n",
    "            skillMatrix.append(0.0)\n",
    "#     print(np.array(skillMatrix))\n",
    "    return np.array(skillMatrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "CareerSkillMatrix = {}\n",
    "for career in JobAndJDs.keys():\n",
    "    SkillMatrix = np.array([0]*len(Skills))\n",
    "    for jd in JobAndJDs[career]:\n",
    "        SkillMatrix = np.add(SkillMatrix,skillinJd(jd,Skills))\n",
    "    SkillMatrix = np.divide(SkillMatrix,len(JobAndJDs[c]))\n",
    "    CareerSkillMatrix[career] = SkillMatrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
